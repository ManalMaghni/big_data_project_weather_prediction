{
  "metadata": {
    "name": "ML",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.hadoop.hbase.HBaseConfiguration\r\n\r\nval conf \u003d HBaseConfiguration.create()\r\nconf.set(\"hbase.zookeeper.quorum\", \"zookeeper\")\r\nconf.set(\"hbase.zookeeper.property.clientPort\", \"2181\")\r\nconf.set(\"zookeeper.znode.parent\", \"/hbase\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.hadoop.hbase.client.Result\r\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\r\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\r\nimport org.apache.hadoop.hbase.util.Bytes\r\nimport org.apache.spark.sql.Row\r\nimport org.apache.spark.sql.types._\r\n\r\n// Définir le schéma de votre table\r\nval schema \u003d StructType(Array(\r\n    StructField(\"rowkey\", StringType, true),\r\n    StructField(\"city\", StringType, true),\r\n    StructField(\"temperature\", DoubleType, true),\r\n    StructField(\"humidity\", IntegerType, true),\r\n    StructField(\"pressure\", IntegerType, true),\r\n    StructField(\"wind_speed\", DoubleType, true),\r\n    StructField(\"weather_main\", StringType, true),\r\n    StructField(\"timestamp\", StringType, true)\r\n))\r\n\r\n// Lire depuis HBase\r\nconf.set(TableInputFormat.INPUT_TABLE, \"weather_data\")\r\nconf.set(TableInputFormat.SCAN_COLUMNS, \"cf:city cf:temperature cf:humidity cf:pressure cf:wind_speed cf:weather_main\")\r\n\r\nval hbaseRDD \u003d sc.newAPIHadoopRDD(\r\n    conf,\r\n    classOf[TableInputFormat],\r\n    classOf[ImmutableBytesWritable],\r\n    classOf[Result]\r\n)\r\n\r\n// Convertir en DataFrame\r\nval rowsRDD \u003d hbaseRDD.map { case (_, result) \u003d\u003e\r\n    val rowkey \u003d Bytes.toString(result.getRow)\r\n    val city \u003d Option(result.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"city\"))).map(b \u003d\u003e Bytes.toString(b)).getOrElse(\"\")\r\n    val temperature \u003d Option(result.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"temperature\"))).map(b \u003d\u003e Bytes.toString(b).toDouble).getOrElse(0.0)\r\n    val humidity \u003d Option(result.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"humidity\"))).map(b \u003d\u003e Bytes.toString(b).toInt).getOrElse(0)\r\n    val pressure \u003d Option(result.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"pressure\"))).map(b \u003d\u003e Bytes.toString(b).toInt).getOrElse(0)\r\n    val wind_speed \u003d Option(result.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"wind_speed\"))).map(b \u003d\u003e Bytes.toString(b).toDouble).getOrElse(0.0)\r\n    val weather_main \u003d Option(result.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"weather_main\"))).map(b \u003d\u003e Bytes.toString(b)).getOrElse(\"\")\r\n    val timestamp \u003d rowkey\r\n    \r\n    Row(rowkey, city, temperature, humidity, pressure, wind_speed, weather_main, timestamp)\r\n}\r\n\r\nval weatherDF \u003d spark.createDataFrame(rowsRDD, schema)\r\n\r\n// Afficher les 10 premières lignes\r\nweatherDF.show(10)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, OneHotEncoder}\r\nimport org.apache.spark.ml.Pipeline\r\nimport org.apache.spark.sql.functions._\r\n\r\n// 1. Convertir les colonnes catégorielles en indices\r\nval indexer \u003d new StringIndexer()\r\n  .setInputCol(\"weather_main\")\r\n  .setOutputCol(\"weather_index\")\r\n\r\n// 2. Encoder One-Hot pour les villes (optionnel)\r\nval cityIndexer \u003d new StringIndexer()\r\n  .setInputCol(\"city\")\r\n  .setOutputCol(\"city_index\")\r\n\r\n// 3. Assembler les features\r\nval assembler \u003d new VectorAssembler()\r\n  .setInputCols(Array(\"humidity\", \"pressure\", \"wind_speed\", \"weather_index\"))\r\n  .setOutputCol(\"features\")\r\n\r\n// 4. Pipeline de prétraitement\r\nval pipeline \u003d new Pipeline()\r\n  .setStages(Array(indexer, cityIndexer, assembler))\r\n\r\nval model \u003d pipeline.fit(weatherDF)\r\nval preparedDF \u003d model.transform(weatherDF)\r\n\r\n// Afficher les données préparées\r\npreparedDF.select(\"temperature\", \"humidity\", \"pressure\", \"wind_speed\", \"weather_main\", \"features\").show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n// Division train/test\r\nval Array(trainingData, testData) \u003d preparedDF.randomSplit(Array(0.8, 0.2), seed \u003d 42)\r\n\r\nprintln(s\"Training: ${trainingData.count()} rows\")\r\nprintln(s\"Test: ${testData.count()} rows\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.ml.regression.LinearRegression\r\n\r\nval lr \u003d new LinearRegression()\r\n  .setLabelCol(\"temperature\")\r\n  .setFeaturesCol(\"features\")\r\n  .setMaxIter(10)\r\n  .setRegParam(0.3)\r\n  .setElasticNetParam(0.8)\r\n\r\nval lrModel \u003d lr.fit(trainingData)\r\nprintln(s\"Linear Regression coefficients: ${lrModel.coefficients}\")\r\nprintln(s\"Linear Regression intercept: ${lrModel.intercept}\")\r\n\r\n// Évaluation\r\nval lrPredictions \u003d lrModel.transform(testData)\r\nlrPredictions.select(\"prediction\", \"temperature\", \"features\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.ml.regression.RandomForestRegressor\r\n\r\nval rf \u003d new RandomForestRegressor()\r\n  .setLabelCol(\"temperature\")\r\n  .setFeaturesCol(\"features\")\r\n  .setNumTrees(10)\r\n\r\nval rfModel \u003d rf.fit(trainingData)\r\nval rfPredictions \u003d rfModel.transform(testData)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.ml.regression.GBTRegressor\r\n\r\nval gbt \u003d new GBTRegressor()\r\n  .setLabelCol(\"temperature\")\r\n  .setFeaturesCol(\"features\")\r\n  .setMaxIter(10)\r\n\r\nval gbtModel \u003d gbt.fit(trainingData)\r\nval gbtPredictions \u003d gbtModel.transform(testData)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\r\n\r\nval evaluator \u003d new RegressionEvaluator()\r\n  .setLabelCol(\"temperature\")\r\n  .setPredictionCol(\"prediction\")\r\n  .setMetricName(\"rmse\")\r\n\r\n// Calculer RMSE pour chaque modèle\r\nval lrRmse \u003d evaluator.evaluate(lrPredictions)\r\nval rfRmse \u003d evaluator.evaluate(rfPredictions)\r\nval gbtRmse \u003d evaluator.evaluate(gbtPredictions) // si vous avez entraîné GBT\r\n\r\nprintln(f\"Linear Regression RMSE: $lrRmse%.2f\")\r\nprintln(f\"Random Forest RMSE: $rfRmse%.2f\")\r\nprintln(f\"GBT RMSE: $gbtRmse%.2f\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n// Sélectionner le modèle avec le plus bas RMSE\r\n//val bestModel \u003d if (lrRmse \u003c rfRmse) lrModel else rfModel\r\n// Comparer les trois RMSE et sélectionner le meilleur modèle\r\nval bestModel \u003d if (lrRmse \u003c\u003d rfRmse \u0026\u0026 lrRmse \u003c\u003d gbtRmse) {\r\n  println(\"Meilleur modèle: Linear Regression\")\r\n  lrModel\r\n} else if (rfRmse \u003c\u003d lrRmse \u0026\u0026 rfRmse \u003c\u003d gbtRmse) {\r\n  println(\"Meilleur modèle: Random Forest\")\r\n  rfModel\r\n} else {\r\n  println(\"Meilleur modèle: GBT\")\r\n  gbtModel\r\n}\r\n// Sauvegarder le modèle\r\nbestModel.write.overwrite().save(\"/models/weather_prediction_model\")\r\n\r\n// Sauvegarder aussi le pipeline de prétraitement\r\nmodel.write.overwrite().save(\"/models/data_preprocessing_pipeline\")\r\n\r\nprintln(\"Modèle sauvegardé avec succès!\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// ------------------------------\n// 1️⃣ Linear Regression\n// ------------------------------\nval lrResults \u003d lrPredictions\n  .select(\"city\", \"temperature\", \"prediction\")\n  .withColumnRenamed(\"prediction\", \"lr_prediction\")\n\nlrResults.show(15)\n\n// ------------------------------\n// 2️⃣ Random Forest\n// ------------------------------\nval rfResults \u003d rfPredictions\n  .select(\"city\", \"temperature\", \"prediction\")\n  .withColumnRenamed(\"prediction\", \"rf_prediction\")\n\nrfResults.show(15)\n\n// ------------------------------\n// 3️⃣ Gradient Boosted Trees\n// ------------------------------\nval gbtResults \u003d gbtPredictions\n  .select(\"city\", \"temperature\", \"prediction\")\n  .withColumnRenamed(\"prediction\", \"gbt_prediction\")\n\ngbtResults.show(15)\n\n// ------------------------------\n// 4️⃣ Fusionner tous les résultats pour comparaison\n// ------------------------------\nval combinedResults \u003d lrResults\n  .join(rfResults.select(\"city\", \"rf_prediction\", \"temperature\"), Seq(\"city\", \"temperature\"))\n  .join(gbtResults.select(\"city\", \"gbt_prediction\", \"temperature\"), Seq(\"city\", \"temperature\"))\n\ncombinedResults.show(15)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// ------------------------------\n// 1️⃣ Linear Regression - supprimer doublons\n// ------------------------------\nval lrResultsUnique \u003d lrPredictions\n  .withColumnRenamed(\"prediction\", \"lr_prediction\")\n  .select(\"city\", \"temperature\", \"lr_prediction\")\n  .dropDuplicates(\"city\", \"temperature\")\n\n//lrResultsUnique.show(15)\nz.show(lrResultsUnique)\n// ------------------------------\n// 2️⃣ Random Forest - supprimer doublons\n// ------------------------------\nval rfResultsUnique \u003d rfPredictions\n  .withColumnRenamed(\"prediction\", \"rf_prediction\")\n  .select(\"city\", \"temperature\", \"rf_prediction\")\n  .dropDuplicates(\"city\", \"temperature\")\n\n//rfResultsUnique.show(15)\nz.show(rfResultsUnique)\n\n// ------------------------------\n// 3️⃣ Gradient Boosted Trees - supprimer doublons\n// ------------------------------\nval gbtResultsUnique \u003d gbtPredictions\n  .withColumnRenamed(\"prediction\", \"gbt_prediction\")\n  .select(\"city\", \"temperature\", \"gbt_prediction\")\n  .dropDuplicates(\"city\", \"temperature\")\n\n//gbtResultsUnique.show(15)\nz.show(gbtResultsUnique)\n// ------------------------------\n// 4️⃣ Fusionner tous les résultats uniques pour comparaison\n// ------------------------------\nval combinedResultsUnique \u003d lrResultsUnique\n  .join(rfResultsUnique, Seq(\"city\", \"temperature\"))\n  .join(gbtResultsUnique, Seq(\"city\", \"temperature\"))\n\n//combinedResultsUnique.show(15)\nz.show(combinedResultsUnique)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval vizDF \u003d combinedResultsUnique.select(\"city\", \"temperature\", \"lr_prediction\", \"rf_prediction\", \"gbt_prediction\")\r\nvizDF.show(20)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n// 1. Calculer l\u0027erreur de chaque modèle\r\nimport org.apache.spark.sql.functions._\r\n\r\nval resultsWithErrors \u003d combinedResultsUnique\r\n  .withColumn(\"lr_error\", abs($\"temperature\" - $\"lr_prediction\"))\r\n  .withColumn(\"rf_error\", abs($\"temperature\" - $\"rf_prediction\"))\r\n  .withColumn(\"gbt_error\", abs($\"temperature\" - $\"gbt_prediction\"))\r\n\r\n// 2. Créer un DataFrame agrégé par ville (idéal pour les graphiques)\r\nval aggregatedByCity \u003d resultsWithErrors.groupBy(\"city\")\r\n  .agg(\r\n    avg(\"temperature\").as(\"avg_actual_temp\"),\r\n    avg(\"lr_error\").as(\"avg_lr_error\"),\r\n    avg(\"rf_error\").as(\"avg_rf_error\"),\r\n    avg(\"gbt_error\").as(\"avg_gbt_error\")\r\n  )\r\n\r\n// Afficher le tableau pour vérification\r\n//aggregatedByCity.show()\r\nz.show(aggregatedByCity)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n// Afficher le DataFrame avec l\u0027interface graphique de Zeppelin\r\nz.show(aggregatedByCity)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n// Convertir un DataFrame en format texte pour %table\r\nval tableOutput \u003d aggregatedByCity\r\n  .orderBy(\"avg_lr_error\") // Trier par exemple\r\n  .collect()\r\n  .map(row \u003d\u003e s\"${row.getAs[String](\"city\")}\\t${row.getAs[Double](\"avg_lr_error\").formatted(\"%.2f\")}\\t${row.getAs[Double](\"avg_rf_error\").formatted(\"%.2f\")}\\t${row.getAs[Double](\"avg_gbt_error\").formatted(\"%.2f\")}\")\r\n  .mkString(\"\\n\")\r\n\r\n// Afficher avec l\u0027en-tête\r\nprint(\"%table Ville\\tErreur LR\\tErreur RF\\tErreur GBT\\n\" + tableOutput)"
    }
  ]
}