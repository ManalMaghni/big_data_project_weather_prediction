{
  "metadata": {
    "name": "StreamingData",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.ml.PipelineModel\r\nimport org.apache.spark.ml.regression._\r\nimport org.apache.spark.ml.feature.VectorAssembler\r\n\r\nval KAFKA_BROKER \u003d \"kafka:29092\"\r\nval INPUT_TOPIC \u003d \"weather-data\"\r\nval MODEL_PATH \u003d \"/models/weather_prediction_model\"\r\n\r\nprintln(\"✅ Configuration chargée\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval schema \u003d new StructType()\r\n  .add(\"city\", StringType)\r\n  .add(\"country\", StringType)\r\n  .add(\"timestamp\", StringType)\r\n  .add(\"temperature\", DoubleType)\r\n  .add(\"feels_like\", DoubleType)\r\n  .add(\"temp_min\", DoubleType)\r\n  .add(\"temp_max\", DoubleType)\r\n  .add(\"pressure\", IntegerType)\r\n  .add(\"humidity\", IntegerType)\r\n  .add(\"weather_main\", StringType)\r\n  .add(\"weather_description\", StringType)\r\n  .add(\"clouds\", IntegerType)\r\n  .add(\"wind_speed\", DoubleType)\r\n  .add(\"wind_deg\", IntegerType)\r\n  .add(\"visibility\", IntegerType)\r\n  .add(\"sunrise\", StringType)\r\n  .add(\"sunset\", StringType)\r\n  .add(\"latitude\", DoubleType)\r\n  .add(\"longitude\", DoubleType)\r\n  .add(\"rain_1h\", DoubleType)\r\n  .add(\"rain_3h\", DoubleType)\r\n  .add(\"snow_1h\", DoubleType)\r\n  .add(\"snow_3h\", DoubleType)\r\n\r\nprintln(\"✅ Schéma défini\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nprintln(\"⏳ Connexion à Kafka...\")\r\n\r\nval dfRaw \u003d spark.readStream\r\n  .format(\"kafka\")\r\n  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\r\n  .option(\"subscribe\", INPUT_TOPIC)\r\n  .option(\"startingOffsets\", \"earliest\")\r\n  .option(\"failOnDataLoss\", \"false\")\r\n  .load()\r\n\r\nval dfParsed \u003d dfRaw\r\n  .selectExpr(\"CAST(value AS STRING) as json_str\")\r\n  .select(from_json(col(\"json_str\"), schema).alias(\"data\"))\r\n  .select(\"data.*\")\r\n\r\nprintln(\"✅ Stream Kafka connecté\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval dfFeatures \u003d dfParsed\n  .withColumn(\"data_timestamp\", to_timestamp(col(\"timestamp\")))\n\nval dfWithTime \u003d dfFeatures\n  .withColumn(\"ts_hour\", hour(col(\"data_timestamp\")))\n  .withColumn(\"ts_day_of_week\", dayofweek(col(\"data_timestamp\")))\n  .withColumn(\"ts_month\", month(col(\"data_timestamp\")))\n\nval dfEngineered \u003d dfWithTime\n  .withColumn(\"is_weekend\", \n    when(col(\"ts_day_of_week\") \u003d\u003d\u003d 1 || col(\"ts_day_of_week\") \u003d\u003d\u003d 7, 1.0).otherwise(0.0))\n  .withColumn(\"is_peak_hour\",\n    when((col(\"ts_hour\") \u003e\u003d 7 \u0026\u0026 col(\"ts_hour\") \u003c 10) || \n         (col(\"ts_hour\") \u003e\u003d 16 \u0026\u0026 col(\"ts_hour\") \u003c 19), 1.0).otherwise(0.0))\n\nval dfFinal \u003d dfEngineered\n  .withColumn(\"is_morning\", \n    when(col(\"ts_hour\") \u003e\u003d 6 \u0026\u0026 col(\"ts_hour\") \u003c 12, 1.0).otherwise(0.0))\n  .withColumn(\"is_afternoon\", \n    when(col(\"ts_hour\") \u003e\u003d 12 \u0026\u0026 col(\"ts_hour\") \u003c 18, 1.0).otherwise(0.0))\n  .withColumn(\"is_evening\", \n    when(col(\"ts_hour\") \u003e\u003d 18, 1.0).otherwise(0.0))\n\nprintln(\"✅ Features créées\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval featureCols \u003d Array(\n  \"humidity\",\n  \"pressure\", \n  \"wind_speed\",\n  \"clouds\",\n  \"visibility\",\n  \"ts_hour\",\n  \"is_weekend\",\n  \"is_peak_hour\",\n  \"is_morning\",\n  \"is_afternoon\",\n  \"is_evening\"\n)\n\nval assembler \u003d new VectorAssembler()\n  .setInputCols(featureCols)\n  .setOutputCol(\"features\")\n\nval dfAssembled \u003d assembler.transform(dfFinal)\n\nprintln(\"✅ Features assemblées\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nimport org.apache.spark.ml.regression.GBTRegressionModel\r\n\r\nprintln(\"📂 Chargement du modèle GBT...\")\r\n\r\nval model \u003d GBTRegressionModel.load(MODEL_PATH)\r\n\r\nprintln(\"✅ Modèle GBT chargé avec succès\")\r\n\r\nval predictions \u003d model.transform(dfAssembled)\r\n\r\nval outputDf \u003d predictions.select(\r\n  col(\"city\"),\r\n  col(\"timestamp\"),\r\n  col(\"temperature\").alias(\"temp_actuelle\"),\r\n  col(\"humidity\"),\r\n  col(\"wind_speed\"),\r\n  col(\"weather_main\"),\r\n  col(\"prediction\").alias(\"temp_predite\"),\r\n  round(abs(col(\"temperature\") - col(\"prediction\")), 2).alias(\"erreur\")\r\n)\r\n\r\nprintln(\"✅ Pipeline de prédiction créé\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nprintln(\"🚀 Démarrage du streaming...\")\r\n\r\nval query \u003d outputDf.writeStream\r\n  .outputMode(\"append\")\r\n  .format(\"console\")\r\n  .option(\"truncate\", false)\r\n  .option(\"numRows\", 10)\r\n  .option(\"checkpointLocation\",\r\n  s\"/tmp/zeppelin/weather_checkpoint\")\r\n  .start()\r\n\r\nprintln(\"✅ Streaming démarré - Les prédictions apparaîtront ci-dessous\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Pour voir les infos du stream\nquery.status\n\n// Pour arrêter le stream après 60 secondes\n Thread.sleep(60000)\n query.stop()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}